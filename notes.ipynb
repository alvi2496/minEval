{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alvi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Processor\n",
    "import numpy\n",
    "import pandas\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "STOPSET_WORDS = ['might', 'may', 'would', 'must', 'lgtm', 'could', 'can', 'good', 'great', 'nice', 'well',\\\n",
    "                 'better', 'worse', 'worst', 'should', 'i', \"i'll\", \"ill\", \"it's\", \"its\", \"im\", \"i'm\", \\\n",
    "                 \"they're\", \"theyre\", \"you're\", \"youre\", \"that's\", 'btw', \"thats\", \"theres\", \"shouldnt\", \\\n",
    "                 \"shouldn't\", \"didn't\", \"didnt\", \"dont\", \"don't\", \"doesn't\", \"doesnt\", \"wasnt\", \"wasn't\", \\\n",
    "                 'sense', \"mon\", 'monday', 'tue', 'wed', 'wednesday', 'thursday', 'thu', 'friday', 'fri', \\\n",
    "                 'sat', 'saturday', 'sun', 'sunday', 'jan', 'january', 'feb', 'february', 'mar', 'march', \\\n",
    "                 'apr', 'april', 'may', 'jun', 'june', 'july', 'jul', 'aug', 'august','sep', 'september', \\\n",
    "                 'oct', 'october', 'nov', 'novenber', 'dec', 'december', 'pm', 'am']\n",
    "\n",
    "WIKI_WORDS = 'app/data/files/wiki-news-300d-1M.vec'\n",
    "encoding=\"utf-8\"\n",
    "\n",
    "def word_embed(data):\n",
    "    # load the pre-trained word-embedding vectors \n",
    "    embeddings_index = {}\n",
    "    for i, line in enumerate(open(WIKI_WORDS)):\n",
    "        values = line.split()\n",
    "        embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "    # create a tokenizer \n",
    "    token = text.Tokenizer()\n",
    "    token.fit_on_texts(data)\n",
    "    word_index = token.word_index\n",
    "    # convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "#     train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "#     valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "    # create token-embedding mapping\n",
    "    embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def input_file_path(url):\n",
    "\treturn url.strip()\n",
    "\n",
    "def structure(data_file_path):\n",
    "\tdata = pandas.read_csv(data_file_path, sep=\",\", header=None, names=['text', 'label'])\n",
    "\treturn data\n",
    "\n",
    "def remove_stopwords(data):\n",
    "\tstopset = set(stopwords.words('english'))\n",
    "\tfor word in STOPSET_WORDS:\n",
    "\t\tstopset.add(word) \n",
    "\t\n",
    "\tdata['text'] = data['text'].apply(lambda sentence: ' '.join([word for word in sentence.lower().split() \\\n",
    "                                                                 if word not in (stopset)]))\n",
    "\treturn data\n",
    "\n",
    "def process(data_url):\n",
    "\treturn remove_stopwords(structure(input_file_path(data_url)))\n",
    "\n",
    "def data_for_evaluation(data, test_size_for_split):\n",
    "\ttrain_data, test_data, train_label, test_label = \\\n",
    "        model_selection.train_test_split(data['text'], \\\n",
    "                                         data['label'], \\\n",
    "                                         test_size=test_size_for_split, \\\n",
    "                                         random_state=0)\n",
    "\treturn train_data, test_data, train_label, test_label\n",
    "\n",
    "def count_vectorize(data, train_data, test_data):\n",
    "\tcount_vector = CountVectorizer(analyzer = 'word', token_pattern = r'\\w{1,}')\n",
    "\tcount_vector.fit(data['text'])\n",
    "\n",
    "# \ttrain_data_count = count_vector.transform(train_data)\n",
    "# \ttest_data_count = count_vector.transform(test_data)\n",
    "\ttrain_data_count = word_embed(train_data)\n",
    "\ttest_data_count = word_embed(test_data)\n",
    "\n",
    "\treturn train_data_count, test_data_count\n",
    "\n",
    "def count_vectorizer(data):\n",
    "\tcount_vect = CountVectorizer()\n",
    "\ttrain_data_count = count_vect.fit_transform(data['text'])\n",
    "\t\n",
    "\treturn train_data_count\n",
    "\n",
    "def tf_idf_vectorize(data, train_data, test_data):\n",
    "\ttf_idf_vector = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "    # for n-gram word vector\n",
    "#     tf_idf_vector = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \\\n",
    "#                                     ngram_range=(2,3), max_features=2738)\n",
    "\ttf_idf_vector.fit(data['text'])\n",
    "\ttrain_data_tf_idf =  tf_idf_vector.transform(train_data)\n",
    "\ttest_data_tf_idf =  tf_idf_vector.transform(test_data)\n",
    "\n",
    "\treturn train_data_tf_idf, test_data_tf_idf\n",
    "\n",
    "def tf_idf_vectorizer(data):\n",
    "\ttf_idf_vector = TfidfVectorizer()\n",
    "\ttrain_data_tf_idf = tf_idf_vector.fit_transform(data['text'])\n",
    "\t\n",
    "\treturn train_data_tf_idf\n",
    "\n",
    "def encode_target_variable(train_label, test_label):\n",
    "\tencoder = preprocessing.LabelEncoder()\n",
    "\ttrain_label = encoder.fit_transform(train_label)\n",
    "\ttest_label = encoder.fit_transform(test_label)\n",
    "\treturn train_label, test_label\n",
    "\n",
    "# data_url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vSGhxSbBeeXdkRdBVQ9wSL1aJTs52SXV3NKfcfoX1wI89XDCJMC5tW0HZk5HYdh2xT0DtufMLSn9hHX/pub?gid=1193567183&single=true&output=csv\"\n",
    "# data = process(data_url)\n",
    "# prepare_word_vector(data['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "def train(classifier, train_data, train_label, is_neural_net=False):\n",
    "    \n",
    "    # fit the training dataset on the classifier\n",
    "    trained_classifier = classifier.fit(train_data, train_label)\n",
    "    return trained_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def split_verifier(trained_classifier, test_data, test_label):\n",
    "\tpredictions = trained_classifier.predict(test_data)\n",
    "\treturn round(metrics.accuracy_score(predictions, test_label), 4)\n",
    "\n",
    "def cross_verifier(classifier, text, label, n):\n",
    "\tresult_array = cross_val_score(classifier, text, label, cv=n)\n",
    "\tresult = sum(result_array) / n\n",
    "\treturn result_array, round(result, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree, naive_bayes, ensemble, svm, linear_model\n",
    "\n",
    "def split_validation(data, test_size_for_split, classifier):\n",
    "\ttrain_data, test_data, train_label, test_label = data_for_evaluation(data, test_size_for_split)\n",
    "\n",
    "\ttrain_data_count, test_data_count = count_vectorize(data, train_data, test_data)\n",
    "\ttrain_data_tf_idf, test_data_tf_idf = tf_idf_vectorize(data, train_data, test_data)\n",
    "\n",
    "\ttrained_classifier_with_count = train(classifier, train_data_count, train_label)\n",
    "\ttrained_classifier_with_tf_idf = train(classifier, train_data_tf_idf, train_label)\n",
    "\n",
    "\tresult_with_count = split_verifier(trained_classifier_with_count, test_data_count, test_label)\n",
    "\tresult_with_tf_idf = split_verifier(trained_classifier_with_tf_idf, test_data_tf_idf, test_label)\n",
    "\n",
    "\treturn result_with_count, result_with_tf_idf\n",
    "\n",
    "def cross_verification(data, n, classifier):\n",
    "\tcount_resut_array, count_result = cross_verifier(classifier, count_vectorizer(data), data['label'], n)\n",
    "\ttf_idf_result_array, tf_idf_result = cross_verifier(classifier, tf_idf_vectorizer(data), data['label'], n)\n",
    "\treturn count_result, tf_idf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "ALGORITHMS = ['Decision Tree', 'Random Forest', 'Naive Bayes', 'SVM', 'Logistic Regression']\n",
    "VERIFICATION_METHODS = ['Split', 'Cross']\n",
    "FEATURE_VECTORS = ['Count', 'TF-IDF']\n",
    "\n",
    "\n",
    "def ctk(value):\n",
    "\tkey = value.lower()\n",
    "\treturn re.sub(\" \", \"_\", key)\n",
    "\n",
    "def initialize():\n",
    "\tresult = {\n",
    "\t\t'algorithms': {\n",
    "\t\t}\n",
    "\t}\n",
    "\tfor a in ALGORITHMS:\n",
    "\t\tresult['algorithms'].update([(ctk(a), {})])\n",
    "\t\tresult['algorithms'][ctk(a)].update([('display_name', a), ('verification_methods', {} )])\n",
    "\t\tfor b in VERIFICATION_METHODS:\n",
    "\t\t\tresult['algorithms'][ctk(a)]['verification_methods'].update([(ctk(b), {})])\n",
    "\t\t\tresult['algorithms'][ctk(a)]['verification_methods'][ctk(b)].update([('display_name', b),('feature_vectors', {})])\n",
    "\t\t\tfor c in FEATURE_VECTORS:\n",
    "\t\t\t\tresult['algorithms'][ctk(a)]['verification_methods'][ctk(b)]['feature_vectors'].update([(ctk(c), {})])\n",
    "\t\t\t\tresult['algorithms'][ctk(a)]['verification_methods'][ctk(b)]['feature_vectors'][ctk(c)].update([('display_name', c), ('value', 0)])\n",
    "\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent(result):\n",
    "\tprint(\"---------------------------------------------------\")\n",
    "\tprint(\"Spliting dataset into train and test\")\n",
    "\tprint(\"***************************************************\")\n",
    "\tfor algo in result['algorithms']:\n",
    "\t\tprint(result['algorithms'][algo]['display_name'])\n",
    "\t\tfor vt in result['algorithms'][algo]['verification_methods']['split']['feature_vectors']:\n",
    "\t\t\tprint(\"With\", result['algorithms'][algo]['verification_methods']['split']['feature_vectors'][vt]['display_name'], \":\", \\\n",
    "\t\t\t result['algorithms'][algo]['verification_methods']['split']['feature_vectors'][vt]['value'])\n",
    "\t\tprint(\"###################################################\")\n",
    "\tprint(\"---------------------------------------------------\")\t\t\n",
    "\n",
    "\tprint(\"---------------------------------------------------\")\n",
    "\tprint(\"Cross validation\")\n",
    "\tprint(\"***************************************************\")\n",
    "\tfor algo in result['algorithms']:\n",
    "\t\tprint(result['algorithms'][algo]['display_name'])\n",
    "\t\tfor vt in result['algorithms'][algo]['verification_methods']['cross']['feature_vectors']:\n",
    "\t\t\tprint(\"With\", result['algorithms'][algo]['verification_methods']['cross']['feature_vectors'][vt]['display_name'], \":\", \\\n",
    "\t\t\t result['algorithms'][algo]['verification_methods']['cross']['feature_vectors'][vt]['value'])\n",
    "\t\tprint(\"###################################################\")\n",
    "\tprint(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autolabel(rects, ax):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height, height, ha='center', va='bottom')        \n",
    "\n",
    "def show_graph(result):\n",
    "\tfig = plt.figure(figsize=(18, 16), dpi= 80)\n",
    "\n",
    "\talgorithm_names = []\n",
    "\tfeature_names = ['Count', 'TF-IDF'] \n",
    "\tfor algo in result['algorithms']:\n",
    "\t\talgorithm_names.append(result['algorithms'][algo]['display_name'])\t\n",
    "\n",
    "\tN = len(result['algorithms'])\n",
    "\tind = np.arange(N) \n",
    "\twidth = .20\n",
    "\n",
    "\tax_1 = fig.add_subplot(211)\n",
    "\n",
    "\tcount_vectors = []\n",
    "\tfor algo in result['algorithms']:\n",
    "\t\tcount_vectors.append(result['algorithms'][algo]['verification_methods']['split']['feature_vectors']['count']['value'])       \n",
    "\n",
    "\trects1 = ax_1.bar(ind, count_vectors, width, color='b')\n",
    "\n",
    "\ttf_idf_vectors = []\n",
    "\tfor algo in result['algorithms']:\n",
    "\t\ttf_idf_vectors.append(result['algorithms'][algo]['verification_methods']['split']['feature_vectors']['tf-idf']['value'])\n",
    "\n",
    "\trects2 = ax_1.bar(ind + width, tf_idf_vectors, width, color='g')\n",
    "\n",
    "\tax_1.set_ylabel('Accuracy')\n",
    "\t# ax_1.set_xlabel('Algotithms grouped by feature of data')\n",
    "\tax_1.set_title('Accuracy using split verification')\n",
    "\tax_1.set_xticks(ind + width / 2)\n",
    "\tax_1.set_xticklabels(algorithm_names)\n",
    "\n",
    "\tax_1.legend((rects1[0], rects2[0]), feature_names)\n",
    "\n",
    "\tautolabel(rects1, ax_1)\n",
    "\tautolabel(rects2, ax_1)\n",
    "\t\n",
    "\tax_2 = fig.add_subplot(212)\n",
    "\n",
    "\tcount_vectors = []\n",
    "\tfor algo in result['algorithms']:\n",
    "\t\tcount_vectors.append(round(result['algorithms'][algo]['verification_methods']['cross']['feature_vectors']['count']['value'], 4))       \n",
    "\n",
    "\trects1 = ax_2.bar(ind, count_vectors, width, color='b')\n",
    "\n",
    "\ttf_idf_vectors = []\n",
    "\tfor algo in result['algorithms']:\n",
    "\t\ttf_idf_vectors.append(round(result['algorithms'][algo]['verification_methods']['cross']['feature_vectors']['tf-idf']['value'], 4))\n",
    "\n",
    "\trects2 = ax_2.bar(ind + width, tf_idf_vectors, width, color='g')\n",
    "\n",
    "\tax_2.set_ylabel('Accuracy')\n",
    "\t# ax_2.set_xlabel('Algotithms grouped by feature of data')\n",
    "\tax_2.set_title('Accuracy using cross verification')\n",
    "\tax_2.set_xticks(ind + width / 2)\n",
    "\tax_2.set_xticklabels(algorithm_names)\n",
    "\n",
    "\tax_2.legend((rects1[0], rects2[0]), feature_names)\n",
    "\n",
    "\tautolabel(rects1, ax_2)\n",
    "\tautolabel(rects2, ax_2)\n",
    "\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(data, test_size_for_split, k_fold_for_cross):\n",
    "\n",
    "\tresult = initialize()\n",
    "\n",
    "\t# Split Verification\n",
    "\tresult['algorithms']['decision_tree']['verification_methods']['split']['feature_vectors']['count']['value'], \\\n",
    "\tresult['algorithms']['decision_tree']['verification_methods']['split']['feature_vectors']['tf-idf']['value'] = \\\n",
    "\tsplit_validation(data, test_size_for_split, tree.DecisionTreeClassifier())\n",
    "\n",
    "\tresult['algorithms']['random_forest']['verification_methods']['split']['feature_vectors']['count']['value'], \\\n",
    "\tresult['algorithms']['random_forest']['verification_methods']['split']['feature_vectors']['tf-idf']['value'] = \\\n",
    "\tsplit_validation(data, test_size_for_split, ensemble.RandomForestClassifier())\n",
    "\n",
    "\tresult['algorithms']['naive_bayes']['verification_methods']['split']['feature_vectors']['count']['value'], \\\n",
    "\tresult['algorithms']['naive_bayes']['verification_methods']['split']['feature_vectors']['tf-idf']['value'] = \\\n",
    "\tsplit_validation(data, test_size_for_split, naive_bayes.MultinomialNB())\n",
    "\n",
    "\tresult['algorithms']['svm']['verification_methods']['split']['feature_vectors']['count']['value'], \\\n",
    "\tresult['algorithms']['svm']['verification_methods']['split']['feature_vectors']['tf-idf']['value'] = \\\n",
    "\tsplit_validation(data, test_size_for_split, svm.SVC())\n",
    "\n",
    "\tresult['algorithms']['logistic_regression']['verification_methods']['split']['feature_vectors']['count']['value'], \\\n",
    "\tresult['algorithms']['logistic_regression']['verification_methods']['split']['feature_vectors']['tf-idf']['value'] = \\\n",
    "\tsplit_validation(data, test_size_for_split, linear_model.LogisticRegression())\n",
    "\n",
    "\t#Cross Verification\n",
    "\tresult['algorithms']['decision_tree']['verification_methods']['cross']['feature_vectors']['count']['value'], \\\n",
    "\tresult['algorithms']['decision_tree']['verification_methods']['cross']['feature_vectors']['tf-idf']['value'] = \\\n",
    "\tcross_verification(data, k_fold_for_cross, tree.DecisionTreeClassifier())\n",
    "\n",
    "\tresult['algorithms']['random_forest']['verification_methods']['cross']['feature_vectors']['count']['value'], \\\n",
    "\tresult['algorithms']['random_forest']['verification_methods']['cross']['feature_vectors']['tf-idf']['value'] = \\\n",
    "\tcross_verification(data, k_fold_for_cross, ensemble.RandomForestClassifier())\n",
    "\n",
    "\tresult['algorithms']['naive_bayes']['verification_methods']['cross']['feature_vectors']['count']['value'], \\\n",
    "\tresult['algorithms']['naive_bayes']['verification_methods']['cross']['feature_vectors']['tf-idf']['value'] = \\\n",
    "\tcross_verification(data, k_fold_for_cross, naive_bayes.MultinomialNB())\n",
    "\n",
    "\tresult['algorithms']['svm']['verification_methods']['cross']['feature_vectors']['count']['value'], \\\n",
    "\tresult['algorithms']['svm']['verification_methods']['cross']['feature_vectors']['tf-idf']['value'] = \\\n",
    "\tcross_verification(data, k_fold_for_cross, svm.SVC())\n",
    "\n",
    "\tresult['algorithms']['logistic_regression']['verification_methods']['cross']['feature_vectors']['count']['value'], \\\n",
    "\tresult['algorithms']['logistic_regression']['verification_methods']['cross']['feature_vectors']['tf-idf']['value'] = \\\n",
    "\tcross_verification(data, k_fold_for_cross, linear_model.LogisticRegression())\n",
    "\trepresent(result), show_graph(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of labels=800 does not match number of samples=2378",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2fcf29d1c4c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## data, test_size_for_split, k_fold_for_cross\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-a58f24501a3e>\u001b[0m in \u001b[0;36mverify\u001b[0;34m(data, test_size_for_split, k_fold_for_cross)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'algorithms'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decision_tree'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verification_methods'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'split'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature_vectors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'algorithms'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decision_tree'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verification_methods'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'split'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature_vectors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tf-idf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0msplit_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size_for_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'algorithms'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'random_forest'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verification_methods'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'split'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature_vectors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-02d393059de4>\u001b[0m in \u001b[0;36msplit_validation\u001b[0;34m(data, test_size_for_split, classifier)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_data_tf_idf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_tf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrained_classifier_with_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtrained_classifier_with_tf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_tf_idf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-2a397cbff717>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(classifier, train_data, train_label, is_neural_net)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# fit the training dataset on the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrained_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyvirenvs/mineval/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyvirenvs/mineval/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             raise ValueError(\"Number of labels=%d does not match \"\n\u001b[0;32m--> 236\u001b[0;31m                              \"number of samples=%d\" % (len(y), n_samples))\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min_weight_fraction_leaf must in [0, 0.5]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of labels=800 does not match number of samples=2378"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vSGhxSbBeeXdkRdBVQ9wSL1aJTs52SXV3NKfcfoX1wI89XDCJMC5tW0HZk5HYdh2xT0DtufMLSn9hHX/pub?gid=1193567183&single=true&output=csv\"\n",
    "# data_url = 'file:///Users/nernst/Documents/projects/design-detect/minEval/data/brunet.csv'\n",
    "data = process(data_url)\n",
    "\n",
    "verify(data, 0.2, 10) ## data, test_size_for_split, k_fold_for_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ref: https://github.com/nadbordrozd/blog_stuff/blob/master/classification_w2v/benchmarking_python3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
