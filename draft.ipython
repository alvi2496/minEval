import numpy
import pandas
import nltk
from nltk.corpus import stopwords
from sklearn import model_selection, preprocessing
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.preprocessing import scale
import numpy as np
import matplotlib.pyplot as plt
from keras.preprocessing import text, sequence

nltk.download('stopwords')

STOPSET_WORDS = ['might', 'may', 'would', 'must', 'lgtm', 'could', 'can', 'good', 'great', 'nice', 'well',\
                 'better', 'worse', 'worst', 'should', 'i', "i'll", "ill", "it's", "its", "im", "i'm", \
                 "they're", "theyre", "you're", "youre", "that's", 'btw', "thats", "theres", "shouldnt", \
                 "shouldn't", "didn't", "didnt", "dont", "don't", "doesn't", "doesnt", "wasnt", "wasn't", \
                 'sense', "mon", 'monday', 'tue', 'wed', 'wednesday', 'thursday', 'thu', 'friday', 'fri', \
                 'sat', 'saturday', 'sun', 'sunday', 'jan', 'january', 'feb', 'february', 'mar', 'march', \
                 'apr', 'april', 'may', 'jun', 'june', 'july', 'jul', 'aug', 'august','sep', 'september', \
                 'oct', 'october', 'nov', 'novenber', 'dec', 'december', 'pm', 'am']

data_url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vSGhxSbBeeXdkRdBVQ9wSL1aJTs52SXV3NKfcfoX1wI89XDCJMC5tW0HZk5HYdh2xT0DtufMLSn9hHX/pub?gid=1193567183&single=true&output=csv"
data_url.strip()
data = pandas.read_csv(data_url, sep=",", header=None, names=['text', 'label'])
stopset = set(stopwords.words('english'))
for word in STOPSET_WORDS:
	stopset.add(word) 
	

data['text'] = data['text'].apply(lambda sentence: ' '.join([word for word in sentence.lower().split() if word not in (stopset)]))
train_data, test_data, train_label, test_label = \
        model_selection.train_test_split(data['text'], \
                                         data['label'], \
                                         test_size=0.2, \
                                         random_state=0)   
count_vector = CountVectorizer(analyzer = 'word', token_pattern = r'\w{1,}')
count_vector.fit(data['text'])  
train_data_count = count_vector.transform(train_data)
test_data_count = count_vector.transform(test_data)

WIKI_WORDS = 'app/data/files/wiki-news-300d-1M.vec'

file_info = open(WIKI_WORDS).readline().split()

embedded_words = {}
for line in open(WIKI_WORDS):
    values = line.split()
    embedded_words[values[0]] = numpy.array(values[1:], dtype = 'float32')

word_vector = np.zeros(np.array((train_data.shape[0], int(file_info[1]))))

i = 0 

for sentence in train_data:
    words = sentence.split()
    length = len(words)
    for word in words:
            if word in embedded_words:
                    word_vector[i] = np.add(word_vector[i], embedded_words[word])
    word_vector[i] = scale(word_vector[i], axis=0, with_mean=True, with_std=True, copy=True)                
    # word_vector[i] = word_vector[i] / length
    i += 1    

